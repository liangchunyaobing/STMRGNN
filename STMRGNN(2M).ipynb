{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Code for Liang, Y., Huang, G., Zhao, Z. (2022). Joint demand prediction for multimodal systems: A multi-task multi-relational spatiotemporal graph neural network approach. Transportation Research Part C: Emerging Technologies, 140, 103731.\n",
        "\n",
        "Part of the code comes from STGCN (https://github.com/hazdzz/STGCN)."
      ],
      "metadata": {
        "id": "npu9R831Lvsw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1oCRYsdJWvY"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import torch.autograd as autograd\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from scipy import sparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzUcj1RPJZme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b17e115e-58f6-426f-dd2b-1753b5f747ab"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKq-Q9vGJg5y"
      },
      "source": [
        "# Adj Processor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa3XaPJ0HLhu"
      },
      "source": [
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "from scipy.linalg import eigvalsh\n",
        "from scipy.linalg import fractional_matrix_power\n",
        "\n",
        "def calculate_random_walk_matrix(adj_mx):\n",
        "    \"\"\"\n",
        "    Returns the random walk adjacency matrix. This is for D_GCN\n",
        "    \"\"\"\n",
        "    adj_mx = sp.coo_matrix(adj_mx)\n",
        "    d = np.array(adj_mx.sum(1))+(1e-5)\n",
        "    d_inv = np.power(d, -1).flatten()\n",
        "    d_inv[np.isinf(d_inv)] = 0.\n",
        "    d_mat_inv = sp.diags(d_inv)\n",
        "    random_walk_mx = d_mat_inv.dot(adj_mx).tocoo()\n",
        "    return random_walk_mx.toarray()\n",
        "\n",
        "def calculate_laplacian_matrix(adj_mat, mat_type):\n",
        "    n_vertex = adj_mat.shape[0]\n",
        "    id_mat = np.asmatrix(np.identity(n_vertex))\n",
        "\n",
        "    # D_row\n",
        "    deg_mat_row = np.asmatrix(np.diag(np.sum(adj_mat, axis=1)))\n",
        "    # D_com\n",
        "    #deg_mat_col = np.asmatrix(np.diag(np.sum(adj_mat, axis=0)))\n",
        "\n",
        "    # D = D_row as default\n",
        "    deg_mat = deg_mat_row\n",
        "    adj_mat = np.asmatrix(adj_mat)\n",
        "\n",
        "    # wid_A = A + I\n",
        "    wid_adj_mat = adj_mat + id_mat\n",
        "    # wid_D = D + I\n",
        "    wid_deg_mat = deg_mat + id_mat\n",
        "\n",
        "    # Combinatorial Laplacian\n",
        "    # L_com = D - A\n",
        "    com_lap_mat = deg_mat - adj_mat\n",
        "\n",
        "    if mat_type == 'id_mat':\n",
        "        return id_mat\n",
        "    elif mat_type == 'com_lap_mat':\n",
        "        return com_lap_mat\n",
        "\n",
        "    if (mat_type == 'sym_normd_lap_mat') or (mat_type == 'wid_sym_normd_lap_mat') or (mat_type == 'hat_sym_normd_lap_mat'):\n",
        "        deg_mat_inv_sqrt = fractional_matrix_power(deg_mat, -0.5)\n",
        "        deg_mat_inv_sqrt[np.isinf(deg_mat_inv_sqrt)] = 0.\n",
        "\n",
        "        wid_deg_mat_inv_sqrt = fractional_matrix_power(wid_deg_mat, -0.5)\n",
        "        wid_deg_mat_inv_sqrt[np.isinf(wid_deg_mat_inv_sqrt)] = 0.\n",
        "\n",
        "        # Symmetric normalized Laplacian\n",
        "        # For SpectraConv\n",
        "        # To [0, 1]\n",
        "        # L_sym = D^{-0.5} * L_com * D^{-0.5} = I - D^{-0.5} * A * D^{-0.5}\n",
        "        sym_normd_lap_mat = np.matmul(np.matmul(deg_mat_inv_sqrt, com_lap_mat), deg_mat_inv_sqrt)\n",
        "\n",
        "        # For ChebConv\n",
        "        # From [0, 1] to [-1, 1]\n",
        "        # wid_L_sym = 2 * L_sym / lambda_max_sym - I\n",
        "        #sym_max_lambda = max(np.linalg.eigvalsh(sym_normd_lap_mat))\n",
        "        sym_max_lambda = max(eigvalsh(sym_normd_lap_mat))\n",
        "        wid_sym_normd_lap_mat = 2 * sym_normd_lap_mat / sym_max_lambda - id_mat\n",
        "\n",
        "        # For GCNConv\n",
        "        # hat_L_sym = wid_D^{-0.5} * wid_A * wid_D^{-0.5}\n",
        "        hat_sym_normd_lap_mat = np.matmul(np.matmul(wid_deg_mat_inv_sqrt, wid_adj_mat), wid_deg_mat_inv_sqrt)\n",
        "\n",
        "        if mat_type == 'sym_normd_lap_mat':\n",
        "            return sym_normd_lap_mat\n",
        "        elif mat_type == 'wid_sym_normd_lap_mat':\n",
        "            return wid_sym_normd_lap_mat\n",
        "        elif mat_type == 'hat_sym_normd_lap_mat':\n",
        "            return hat_sym_normd_lap_mat\n",
        "\n",
        "    elif (mat_type == 'rw_normd_lap_mat') or (mat_type == 'wid_rw_normd_lap_mat') or (mat_type == 'hat_rw_normd_lap_mat'):\n",
        "        try:\n",
        "            # There is a small possibility that the degree matrix is a singular matrix.\n",
        "            deg_mat_inv = np.linalg.inv(deg_mat)\n",
        "        except:\n",
        "            print(f'The degree matrix is a singular matrix. Cannot use random walk normalized Laplacian matrix.')\n",
        "        else:\n",
        "            deg_mat_inv[np.isinf(deg_mat_inv)] = 0.\n",
        "\n",
        "        wid_deg_mat_inv = np.linalg.inv(wid_deg_mat)\n",
        "        wid_deg_mat_inv[np.isinf(wid_deg_mat_inv)] = 0.\n",
        "\n",
        "        # Random Walk normalized Laplacian\n",
        "        # For SpectraConv\n",
        "        # To [0, 1]\n",
        "        # L_rw = D^{-1} * L_com = I - D^{-1} * A\n",
        "        rw_normd_lap_mat = np.matmul(deg_mat_inv, com_lap_mat)\n",
        "\n",
        "        # For ChebConv\n",
        "        # From [0, 1] to [-1, 1]\n",
        "        # wid_L_rw = 2 * L_rw / lambda_max_rw - I\n",
        "        #rw_max_lambda = max(np.linalg.eigvalsh(rw_normd_lap_mat))\n",
        "        rw_max_lambda = max(eigvalsh(rw_normd_lap_mat))\n",
        "        wid_rw_normd_lap_mat = 2 * rw_normd_lap_mat / rw_max_lambda - id_mat\n",
        "\n",
        "        # For GCNConv\n",
        "        # hat_L_rw = wid_D^{-1} * wid_A\n",
        "        hat_rw_normd_lap_mat = np.matmul(wid_deg_mat_inv, wid_adj_mat)\n",
        "\n",
        "        if mat_type == 'rw_normd_lap_mat':\n",
        "            return rw_normd_lap_mat\n",
        "        elif mat_type == 'wid_rw_normd_lap_mat':\n",
        "            return wid_rw_normd_lap_mat\n",
        "        elif mat_type == 'hat_rw_normd_lap_mat':\n",
        "            return hat_rw_normd_lap_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUKW0JR8TxEh"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3fCmPDYJRwQ"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "\n",
        "class Align(nn.Module):\n",
        "    def __init__(self, c_in, c_out):\n",
        "        super(Align, self).__init__()\n",
        "        self.c_in = c_in\n",
        "        self.c_out = c_out\n",
        "        self.align_conv = nn.Conv2d(in_channels=c_in, out_channels=c_out, kernel_size=(1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.c_in > self.c_out:\n",
        "            x_align = self.align_conv(x)\n",
        "        elif self.c_in < self.c_out:\n",
        "            batch_size, c_in, timestep, n_vertex = x.shape\n",
        "            x_align = torch.cat([x, torch.zeros([batch_size, self.c_out - self.c_in, timestep, n_vertex]).to(x)], dim=1)\n",
        "        else:\n",
        "            x_align = x\n",
        "        \n",
        "        return x_align\n",
        "\n",
        "class CausalConv1d(nn.Conv1d):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, enable_padding=False, dilation=1, groups=1, bias=True):\n",
        "        if enable_padding == True:\n",
        "            self.__padding = (kernel_size - 1) * dilation\n",
        "        else:\n",
        "            self.__padding = 0\n",
        "        super(CausalConv1d, self).__init__(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=self.__padding, dilation=dilation, groups=groups, bias=bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        result = super(CausalConv1d, self).forward(input)\n",
        "        if self.__padding != 0:\n",
        "            return result[: , : , : -self.__padding]\n",
        "        \n",
        "        return result\n",
        "\n",
        "class CausalConv2d(nn.Conv2d):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, enable_padding=False, dilation=1, groups=1, bias=True):\n",
        "        kernel_size = nn.modules.utils._pair(kernel_size)\n",
        "        stride = nn.modules.utils._pair(stride)\n",
        "        dilation = nn.modules.utils._pair(dilation)\n",
        "        if enable_padding == True:\n",
        "            self.__padding = [int((kernel_size[i] - 1) * dilation[i]) for i in range(len(kernel_size))]\n",
        "        else:\n",
        "            self.__padding = 0\n",
        "        self.left_padding = nn.modules.utils._pair(self.__padding)\n",
        "        super(CausalConv2d, self).__init__(in_channels, out_channels, kernel_size, stride=stride, padding=0, dilation=dilation, groups=groups, bias=bias)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        if self.__padding != 0:\n",
        "            input = F.pad(input, (self.left_padding[1], 0, self.left_padding[0], 0))\n",
        "        result = super(CausalConv2d, self).forward(input)\n",
        "\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rInWO9cwJn1K"
      },
      "source": [
        "# Temporal Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmYrZe8iJsGM"
      },
      "source": [
        "class TemporalConvLayer(nn.Module):\n",
        "\n",
        "    # Temporal Convolution Layer (GLU)\n",
        "    #\n",
        "    #        |-------------------------------| * residual connection *\n",
        "    #        |                               |\n",
        "    #        |    |--->--- casual conv ----- + -------|       \n",
        "    # -------|----|                                   ⊙ ------>\n",
        "    #             |--->--- casual conv --- sigmoid ---|                               \n",
        "    #\n",
        "    \n",
        "    #param x: tensor, [batch_size, c_in, timestep, n_vertex]\n",
        "\n",
        "    def __init__(self, Kt, c_in, c_out, n_vertex, act_func, enable_gated_act_func):\n",
        "        super(TemporalConvLayer, self).__init__()\n",
        "        self.Kt = Kt\n",
        "        self.c_in = c_in\n",
        "        self.c_out = c_out\n",
        "        self.n_vertex = n_vertex\n",
        "        self.act_func = act_func\n",
        "        self.enable_gated_act_func = enable_gated_act_func\n",
        "        self.align = Align(c_in, c_out)\n",
        "        if enable_gated_act_func == True:\n",
        "            self.causal_conv = CausalConv2d(in_channels=c_in, out_channels=2 * c_out, kernel_size=(Kt, 1), enable_padding=False, dilation=1)\n",
        "        else:\n",
        "            self.causal_conv = CausalConv2d(in_channels=c_in, out_channels=c_out, kernel_size=(Kt, 1), enable_padding=False, dilation=1)\n",
        "        self.linear = nn.Linear(n_vertex, n_vertex)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "        self.elu = nn.ELU()\n",
        "\n",
        "    def forward(self, x):   \n",
        "        x_in = self.align(x)[:, :, self.Kt - 1:, :]\n",
        "        x_causal_conv = self.causal_conv(x)\n",
        "\n",
        "        if self.enable_gated_act_func == True:\n",
        "            x_p = x_causal_conv[:, : self.c_out, :, :]\n",
        "            x_q = x_causal_conv[:, -self.c_out:, :, :]\n",
        "\n",
        "            # Temporal Convolution Layer (GLU)\n",
        "            if self.act_func == 'glu':\n",
        "                # GLU was first purposed in\n",
        "                # Language Modeling with Gated Convolutional Networks\n",
        "                # https://arxiv.org/abs/1612.08083\n",
        "                # Input tensor X was split by a certain dimension into tensor X_a and X_b\n",
        "                # In original paper, GLU as Linear(X_a) ⊙ Sigmoid(Linear(X_b))\n",
        "                # However, in PyTorch, GLU as X_a ⊙ Sigmoid(X_b)\n",
        "                # https://pytorch.org/docs/master/nn.functional.html#torch.nn.functional.glu\n",
        "                # Because in original paper, the representation of GLU and GTU is ambiguous\n",
        "                # So, it is arguable which one version is correct\n",
        "\n",
        "                # (x_p + x_in) ⊙ Sigmoid(x_q)\n",
        "                x_glu = torch.mul((x_p + x_in), self.sigmoid(x_q))\n",
        "                x_tc_out = x_glu\n",
        "\n",
        "            # Temporal Convolution Layer (GTU)\n",
        "            elif self.act_func == 'gtu':\n",
        "                # Tanh(x_p + x_in) ⊙ Sigmoid(x_q)\n",
        "                x_gtu = torch.mul(self.tanh(x_p + x_in), self.sigmoid(x_q))\n",
        "                x_tc_out = x_gtu\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f'ERROR: activation function {self.act_func} is not defined.')\n",
        "\n",
        "        else:\n",
        "            # Temporal Convolution Layer (Linear)\n",
        "            if self.act_func == 'linear':\n",
        "                x_linear = self.linear(x_causal_conv + x_in)\n",
        "                x_tc_out = x_linear\n",
        "            \n",
        "            # Temporal Convolution Layer (Sigmoid)\n",
        "            elif self.act_func == 'sigmoid':\n",
        "                x_sigmoid = self.sigmoid(x_causal_conv + x_in)\n",
        "                x_tc_out = x_sigmoid\n",
        "\n",
        "            # Temporal Convolution Layer (Tanh)\n",
        "            elif self.act_func == 'tanh':\n",
        "                x_tanh = self.tanh(x_causal_conv + x_in)\n",
        "                x_tc_out = x_tanh\n",
        "\n",
        "            # Temporal Convolution Layer (ReLU)\n",
        "            elif self.act_func == 'relu':\n",
        "                x_relu = self.relu(x_causal_conv + x_in)\n",
        "                x_tc_out = x_relu\n",
        "        \n",
        "            # Temporal Convolution Layer (LeakyReLU)\n",
        "            elif self.act_func == 'leaky_relu':\n",
        "                x_leaky_relu = self.leaky_relu(x_causal_conv + x_in)\n",
        "                x_tc_out = x_leaky_relu\n",
        "\n",
        "            # Temporal Convolution Layer (ELU)\n",
        "            elif self.act_func == 'elu':\n",
        "                x_elu = self.elu(x_causal_conv + x_in)\n",
        "                x_tc_out = x_elu\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f'ERROR: activation function {self.act_func} is not defined.')\n",
        "        return x_tc_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRmyiaApk2CE"
      },
      "source": [
        "# Relation Aggregation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCmNPmSdlAUo"
      },
      "source": [
        "class AttnAgg(nn.Module):\n",
        "     def __init__(self, embed_dim): \n",
        "        super(AttnAgg, self).__init__()\n",
        "        self.attn = nn.Linear(embed_dim, embed_dim)\n",
        "        self.attn_relu = nn.ReLU()\n",
        "        self.attn_weight = nn.Linear(embed_dim, 1)\n",
        "        \n",
        "     def forward(self, input):\n",
        "        input = input.permute(0, 1, 3, 4, 2)\n",
        "        K, batch_size, T, n_node, c_in = input.shape\n",
        "        x_gc_first_attn = input.reshape(K, -1, c_in)\n",
        "        attn = F.softmax(self.attn_weight(x_gc_first_attn).permute(1, 2, 0), -1)\n",
        "        x_gc_second_attn = torch.bmm(attn, x_gc_first_attn.permute(1, 0, 2)).squeeze(1) # [B, 1, self.c_out]\n",
        "        agg_output = x_gc_second_attn.reshape(batch_size, T, n_node, c_in).permute(0, 3, 1, 2)\n",
        "        return agg_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJbDSzIMKF0P"
      },
      "source": [
        "# Spatial Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioSe0LBiKKBY"
      },
      "source": [
        "class MGCNConv(nn.Module):\n",
        "    def __init__(self, c_in, c_out, enable_bias, graph_conv_act_func, K=2):\n",
        "        super(MGCNConv, self).__init__()\n",
        "        self.c_in = c_in\n",
        "        self.c_out = c_out\n",
        "        self.enable_bias = enable_bias\n",
        "        self.graph_conv_act_func = graph_conv_act_func\n",
        "        # self.out = nn.Linear(c_in*K, c_out)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.K = K\n",
        "        self.weight = nn.Parameter(torch.cuda.FloatTensor(K, c_in, c_out))\n",
        "        if enable_bias == True:\n",
        "            self.bias = nn.Parameter(torch.cuda.FloatTensor(c_out))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        # For Sigmoid or Tanh\n",
        "        if self.graph_conv_act_func == 'sigmoid' or self.graph_conv_act_func == 'tanh':\n",
        "            init.xavier_uniform_(tensor=self.weight, gain=init.calculate_gain(self.graph_conv_act_func))\n",
        "\n",
        "        # For ReLU, LeakyReLU, or ELU\n",
        "        elif self.graph_conv_act_func == 'relu' or self.graph_conv_act_func == 'leaky_relu' \\\n",
        "            or self.graph_conv_act_func == 'elu':\n",
        "            init.kaiming_uniform_(self.weight)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, x, gcnconv_matrix):\n",
        "        batch_size, c_in, T, n_vertex = x.shape\n",
        "        n_matrix = gcnconv_matrix.shape[0]\n",
        "        x_first_mul = torch.einsum('ab, cbd->cad', x.reshape(-1, c_in), self.weight).view(self.K, batch_size*T, n_vertex, -1) # [batch, c_in], [K, c_in, c_out] -> [K, batch, c_out]\n",
        "        x_second_mul = torch.einsum('ecab,ecbd->ecad', gcnconv_matrix, x_first_mul) # [n_adj, B*T, n_node1, n_node2] * [n_adj, batch_size * T, n_node2, c_out] -> [n_adj, B*T, n_node1, c_out]\n",
        "        if self.bias is not None:\n",
        "            x_gcnconv = x_second_mul + self.bias\n",
        "        else:\n",
        "            x_gcnconv = x_second_mul\n",
        "        x_gcnconv = x_gcnconv.view(n_matrix, batch_size, T, -1, self.c_out).permute(0, 1, 4, 2, 3) # [n_adj, batch_size, T, n_node1, c_out]\n",
        "        return self.relu(x_gcnconv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfoVFdFHKgeF"
      },
      "source": [
        "class GraphConvLayer(nn.Module):\n",
        "    def __init__(self, Ks, c_in, c_out, graph_conv_type, graph_conv_matrix, graph_conv_act_func, mode):\n",
        "        super(GraphConvLayer, self).__init__()\n",
        "        self.Ks = Ks\n",
        "        self.c_in = c_in\n",
        "        self.c_out = c_out\n",
        "        self.align = Align(c_in, c_out)\n",
        "        self.graph_conv_type = graph_conv_type\n",
        "        self.graph_conv_matrix = graph_conv_matrix\n",
        "        self.graph_conv_act_func = graph_conv_act_func\n",
        "        self.enable_bias = True\n",
        "        self.mode = mode\n",
        "\n",
        "        self.intra_gcnconv = MGCNConv(c_out, c_out, self.enable_bias, graph_conv_act_func)\n",
        "        self.inter_gcnconv = MGCNConv(c_out, c_out, self.enable_bias, graph_conv_act_func)\n",
        "        self.rel_agg = AttnAgg(c_out)\n",
        "\n",
        "\n",
        "    def forward(self, x, x_inter):\n",
        "        x_in = self.align(x)\n",
        "        batch_size, c_in, T, n_node1 = x_in.shape # n-vertex: n_node2\n",
        "        # print(x_in.shape)\n",
        "\n",
        "        x_inter_in = self.align(x_inter)\n",
        "        batch_size, c_in, T, n_node2 = x_inter_in.shape # n-vertex: n_node2\n",
        "\n",
        "        intra_adj = self.graph_conv_matrix[0].unsqueeze(1).repeat(1, batch_size*T, 1, 1) #[2, batch_size*T, n_node1, n_node2]\n",
        "        inter_adj = self.graph_conv_matrix[1].unsqueeze(1).repeat(1, batch_size*T, 1, 1) # [2, batch_size*T, n_node1, n_node2]\n",
        "        x_intra = self.intra_gcnconv(x_in, intra_adj)\n",
        "        x_inter = self.inter_gcnconv(x_inter_in, inter_adj)\n",
        "\n",
        "        x_gc_with_rc = torch.cat([x_intra, x_inter], 0)\n",
        "        x_gc_with_rc = self.rel_agg(x_gc_with_rc)\n",
        "        # x_gc_with_rc = torch.add(x_intra, x_inter)\n",
        "        x_gc_out = torch.add(x_gc_with_rc, x_in)\n",
        "\n",
        "        return x_gc_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSaXBTZaKNjw"
      },
      "source": [
        "# ST-BLOCK & OUTPUT BLOCK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29gihtadKQLZ"
      },
      "source": [
        "class STConvBlock(nn.Module):\n",
        "    # STConv Block contains 'TGTND' structure\n",
        "    # T: Gated Temporal Convolution Layer (GLU or GTU)\n",
        "    # G: Graph Convolution Layer (ChebConv or GCNConv)\n",
        "    # T: Gated Temporal Convolution Layer (GLU or GTU)\n",
        "    # N: Layer Normolization\n",
        "    # D: Dropout\n",
        "\n",
        "    def __init__(self, Kt, Ks, n_node1, n_node2, last_block_channel, channels, gated_act_func, graph_conv_type, graph_conv_matrix, drop_rate):\n",
        "        super(STConvBlock, self).__init__()\n",
        "        self.Kt = Kt\n",
        "        self.Ks = Ks\n",
        "\n",
        "        self.n_node1 = n_node1\n",
        "        self.n_node2 = n_node2\n",
        "\n",
        "        self.last_block_channel = last_block_channel\n",
        "        self.channels = channels\n",
        "        self.gated_act_func = gated_act_func\n",
        "        self.enable_gated_act_func = True\n",
        "        self.graph_conv_type = graph_conv_type\n",
        "        self.graph_conv_matrix = graph_conv_matrix\n",
        "        self.graph_conv_act_func = 'relu'\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "        self.tmp_conv11 = TemporalConvLayer(Kt, last_block_channel, channels[0], n_node1, gated_act_func, self.enable_gated_act_func)\n",
        "        self.graph_conv1 = GraphConvLayer(Ks, channels[0], channels[1], graph_conv_type, graph_conv_matrix[:2], self.graph_conv_act_func, 'subway')\n",
        "        self.tmp_conv12 = TemporalConvLayer(Kt, channels[1], channels[2], n_node1, gated_act_func, self.enable_gated_act_func)\n",
        "\n",
        "        self.tmp_conv21 = TemporalConvLayer(Kt, last_block_channel, channels[0], n_node2, gated_act_func, self.enable_gated_act_func)\n",
        "        self.graph_conv2 = GraphConvLayer(Ks, channels[0], channels[1], graph_conv_type, graph_conv_matrix[2:], self.graph_conv_act_func, 'taxi')\n",
        "        self.tmp_conv22 = TemporalConvLayer(Kt, channels[1], channels[2], n_node2, gated_act_func, self.enable_gated_act_func)\n",
        "        \n",
        "        self.tc1_ln = nn.LayerNorm([n_node1, channels[2]])\n",
        "        self.tc2_ln = nn.LayerNorm([n_node2, channels[2]])\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "        self.elu = nn.ELU()\n",
        "        self.do = nn.Dropout(p=drop_rate)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # print('x1', x1.shape, 'x2', x2.shape)\n",
        "        x_tmp_conv11 = self.tmp_conv11(x1)\n",
        "        x_tmp_conv21 = self.tmp_conv21(x2)\n",
        "        # print('x_tmp_conv11', x_tmp_conv11.shape)\n",
        "        # print('x_tmp_conv21', x_tmp_conv21.shape)\n",
        "\n",
        "        x_graph_conv1 = self.graph_conv1(x_tmp_conv11, x_tmp_conv21) # [n_adj, batch_size, T, n_node1, c_out]\n",
        "        x_graph_conv2 = self.graph_conv2(x_tmp_conv21, x_tmp_conv11) # [n_adj, batch_size, T, n_node1, c_out]\n",
        "        # print('x_graph_conv1', x_graph_conv1.shape)\n",
        "        # print('x_graph_conv2', x_graph_conv2.shape)\n",
        "        \n",
        "        x_act_func1 = self.relu(x_graph_conv1)\n",
        "        x_act_func2 = self.relu(x_graph_conv2)\n",
        "\n",
        "        x_tmp_conv12 = self.tmp_conv12(x_act_func1)\n",
        "        x_tmp_conv22 = self.tmp_conv22(x_act_func2)\n",
        "\n",
        "        x_tc1_ln = self.tc1_ln(x_tmp_conv12.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
        "        x_tc2_ln = self.tc2_ln(x_tmp_conv22.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
        "\n",
        "        x_do1 = self.do(x_tc1_ln)\n",
        "        x_do2 = self.do(x_tc2_ln)\n",
        "\n",
        "        return x_do1, x_do2\n",
        "\n",
        "class OutputBlock(nn.Module):\n",
        "    # Output block contains 'TNFF' structure\n",
        "    # T: Gated Temporal Convolution Layer (GLU or GTU)\n",
        "    # N: Layer Normolization\n",
        "    # F: Fully-Connected Layer\n",
        "    # F: Fully-Connected Layer\n",
        "\n",
        "    def __init__(self, Ko, last_block_channel, channels, end_channel, n_vertex, gated_act_func, drop_rate):\n",
        "        super(OutputBlock, self).__init__()\n",
        "        self.Ko = Ko\n",
        "        self.last_block_channel = last_block_channel\n",
        "        self.channels = channels\n",
        "        self.end_channel = end_channel\n",
        "        self.n_vertex = n_vertex\n",
        "        self.gated_act_func = gated_act_func\n",
        "        self.enable_gated_act_func = True\n",
        "        self.drop_rate = drop_rate\n",
        "        self.tmp_conv1 = TemporalConvLayer(Ko, last_block_channel, channels[0], n_vertex, gated_act_func, self.enable_gated_act_func)\n",
        "        self.fc1 = nn.Linear(channels[0], channels[1])\n",
        "        self.fc2 = nn.Linear(channels[1], end_channel)\n",
        "        self.tc1_ln = nn.LayerNorm([n_vertex, channels[0]])\n",
        "        self.act_func = 'sigmoid'\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "        self.elu = nn.ELU()\n",
        "        self.do = nn.Dropout(p=self.drop_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_tc1 = self.tmp_conv1(x)\n",
        "        x_tc1_ln = self.tc1_ln(x_tc1.permute(0, 2, 3, 1))\n",
        "        x_fc1 = self.fc1(x_tc1_ln)\n",
        "        if self.act_func == 'sigmoid':\n",
        "            x_act_func = self.sigmoid(x_fc1)\n",
        "        elif self.act_func == 'tanh':\n",
        "            x_act_func = self.tanh(x_fc1)\n",
        "        elif self.act_func == 'relu':\n",
        "            x_act_func = self.relu(x_fc1)\n",
        "        elif self.act_func == 'leaky_relu':\n",
        "            x_act_func = self.leaky_relu(x_fc1)\n",
        "        elif self.act_func == 'elu':\n",
        "            x_act_func = self.elu(x_fc1)\n",
        "        x_fc2 = self.fc2(x_act_func).permute(0, 3, 1, 2)\n",
        "        x_out = x_fc2\n",
        "\n",
        "        return x_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZT7T75dKtIt"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VI6ymAZKpHt"
      },
      "source": [
        "class mySequential(nn.Sequential):\n",
        "    def forward(self, *input):\n",
        "        for module in self._modules.values():\n",
        "            input = module(*input)\n",
        "        return input\n",
        "\n",
        "class STGCN_ChebConv(nn.Module):\n",
        "    def __init__(self, Kt, Ks, blocks, T, n_node1, n_node2, gated_act_func, graph_conv_type, graph_conv_matrix, drop_rate):\n",
        "        super(STGCN_ChebConv, self).__init__()\n",
        "        self.n_node1 = n_node1\n",
        "        self.n_node2 = n_node2\n",
        "        modules = []\n",
        "\n",
        "        # use to compute adaptive adj matrix\n",
        "\n",
        "        # self.graph_conv_matrix = graph_conv_matrix\n",
        "        for l in range(len(blocks) - 3):\n",
        "            modules.append(STConvBlock(Kt, Ks, n_node1, n_node2, blocks[l][-1], blocks[l+1], gated_act_func, graph_conv_type, graph_conv_matrix, drop_rate))\n",
        "        self.st_blocks = mySequential(*modules)\n",
        "\n",
        "        # Ko = T - (len(blocks) - 3) * 2 * (Kt - 1)\n",
        "        Ko = T - (len(blocks) - 3) * (Kt - 1)\n",
        "        self.Ko = Ko\n",
        "        print(Ko)\n",
        "        if self.Ko > 1:\n",
        "            self.output1 = OutputBlock(Ko, blocks[-3][-1], blocks[-2], blocks[-1][0], n_node1, gated_act_func, drop_rate)\n",
        "            self.output2 = OutputBlock(Ko, blocks[-3][-1], blocks[-2], blocks[-1][0], n_node2, gated_act_func, drop_rate)\n",
        "        elif self.Ko <= 1:\n",
        "            self.fc1 = nn.Linear(blocks[-3][-1], blocks[-2][0])\n",
        "            self.fc2 = nn.Linear(blocks[-2][0], blocks[-1][0])\n",
        "            self.act_func = 'sigmoid'\n",
        "            self.sigmoid = nn.Sigmoid()\n",
        "            self.tanh = nn.Tanh()\n",
        "            self.relu = nn.ReLU()\n",
        "            self.leaky_relu = nn.LeakyReLU()\n",
        "            self.elu = nn.ELU()\n",
        "            self.do = nn.Dropout(p=drop_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 3, 1, 2)   # batch_size, c_in, T, n_vertex \n",
        "        x1, x2 = x[:, :, :, :self.n_node1], x[:, :, :, self.n_node1:]\n",
        "\n",
        "        x_st1, x_st2 = self.st_blocks(x1, x2)\n",
        "        \n",
        "        x_out1 = self.output1(x_st1).permute(0, 2, 3, 1)\n",
        "        x_out2 = self.output2(x_st2).permute(0, 2, 3, 1)\n",
        "        \n",
        "        return x_out1, x_out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDCLLOfcTxHB"
      },
      "source": [
        "# Data Container"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y9CVjWmKDKL"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "\n",
        "class DataInput(object):\n",
        "    def __init__(self, data_dir1, data_dir2, norm_opt=True):\n",
        "        self.data_dir1 = data_dir1\n",
        "        self.data_dir2 = data_dir2\n",
        "        self.min1, self.max1 = 0, 0\n",
        "        self.min2, self.max2 = 0, 0\n",
        "        self.n_node1, self.n_node2 = 0, 0\n",
        "        self.norm_opt = norm_opt\n",
        "\n",
        "    def load_data(self):\n",
        "        print('Loading data...')\n",
        "        dataset = dict()\n",
        "        data1 = np.load(self.data_dir1)\n",
        "        print(data1.shape)\n",
        "        self.min1, self.max1, self.n_node1 = data1.min(), data1.max(), data1.shape[1]\n",
        "        print(self.min1, self.max1, self.n_node1)\n",
        "        data1 = self.minmax_normalize(data1, self.min1, self.max1) if self.norm_opt else data\n",
        "        \n",
        "        data2 = np.load(self.data_dir2)\n",
        "        print(data2.shape)\n",
        "        self.min2, self.max2, self.n_node2 = data2.min(), data2.max(), data2.shape[1]\n",
        "        print(self.min2, self.max2, self.n_node2)\n",
        "        data2 = self.minmax_normalize(data2, self.min2, self.max2) if self.norm_opt else data\n",
        "\n",
        "        dataset['concat'] = np.concatenate([data1, data2], 1)\n",
        "        return dataset\n",
        "\n",
        "    def minmax_normalize(self, x:np.array, min, max):\n",
        "        x = (x - min) / (max - min)\n",
        "        x = 2 * x - 1\n",
        "        return x\n",
        "\n",
        "    def minmax_denormalize(self, x:np.array, min, max):\n",
        "        x = (x + 1)/2\n",
        "        x = (max - min) * x + min\n",
        "        return x\n",
        "\n",
        "class TaxiDataset(Dataset):\n",
        "    '''\n",
        "        inputs: history obs: short-term seq | daily seq | weekly seq (B, seq, N, C)\n",
        "        output: y_t+1 target (B, N, C)\n",
        "        mode: one in [train, validate, test]\n",
        "        mode_len: {train, validate, test}\n",
        "    '''\n",
        "    def __init__(self, device:str, inputs:dict, output:np.array, mode:str, mode_len:dict, start_idx:int):\n",
        "        self.device = device\n",
        "        self.mode = mode\n",
        "        self.mode_len = mode_len\n",
        "        self.start_idx = start_idx  # train_start idx\n",
        "        self.inputs, self.output = self.prepare_xy(inputs, output)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.mode_len[self.mode]\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.inputs['x_seq'][item], self.output[item]\n",
        "\n",
        "    # to solve tomorrow morning: I think there is some problem with the start_idx it sets. It sets it as day rather than time intervals.\n",
        "    def prepare_xy(self, inputs:dict, output:np.array):\n",
        "        if self.mode == 'train':\n",
        "            pass\n",
        "        elif self.mode == 'validate':\n",
        "            self.start_idx += self.mode_len['train']\n",
        "        else:       # test\n",
        "            self.start_idx += (self.mode_len['train'] + self.mode_len['validate'])\n",
        "\n",
        "        obs = []\n",
        "        for kw in ['weekly', 'daily', 'serial']:\n",
        "            if len(inputs[kw].shape) != 2:      # dim=2 for empty seq\n",
        "                obs.append(inputs[kw])\n",
        "        x_seq = np.concatenate(obs, axis=1)   # concatenate timeslices to one seq\n",
        "        x = dict()\n",
        "        x['x_seq'] = torch.from_numpy(x_seq[self.start_idx :  (self.start_idx + self.mode_len[self.mode])]).float().to(self.device)\n",
        "        y = torch.from_numpy(output[self.start_idx : self.start_idx + self.mode_len[self.mode]]).float().to(self.device)\n",
        "        return x, y\n",
        "\n",
        "class DataGenerator(object):\n",
        "    def __init__(self, dt:int, obs_len:tuple, pre_horizon:int):\n",
        "        self.day_timesteps = int(24*dt)\n",
        "        self.serial_len, self.daily_len, self.weekly_len = obs_len\n",
        "        self.pre_horizon = pre_horizon\n",
        "        self.start_idx = 0\n",
        "        self.mode_len = dict()\n",
        "        # self.start_idx, self.mode_len = self.date2len(data_len=data_len)\n",
        "\n",
        "    def date2len(self, data_len:int):\n",
        "        d_len = data_len - self.pre_horizon + 1\n",
        "        day_count = d_len//self.day_timesteps\n",
        "        split_line1 = int(day_count * 0.6) * self.day_timesteps\n",
        "        split_line2 = int(day_count * 0.8) * self.day_timesteps\n",
        "        return 0, {'train':split_line1, 'validate':split_line2-split_line1, 'test':d_len-split_line2}\n",
        "\n",
        "    def get_data_loader(self, data:dict, batch_size:int, device:str):\n",
        "        feat_dict = dict()\n",
        "        feat_dict['serial'], feat_dict['daily'], feat_dict['weekly'], output = self.get_feats(data['concat'])\n",
        "\n",
        "        print(feat_dict['serial'].shape)\n",
        "        data_len = feat_dict['serial'].shape[0]\n",
        "        self.start_idx, self.mode_len = self.date2len(data_len)\n",
        "        data_loader = dict() # data_loader for [train, validate, test]\n",
        "        for mode in ['train', 'validate', 'test']:\n",
        "            dataset = TaxiDataset(device=device, inputs=feat_dict, output=output,\n",
        "                                  mode=mode, mode_len=self.mode_len, start_idx=self.start_idx)\n",
        "            data_loader[mode] = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False)\n",
        "        return data_loader\n",
        "\n",
        "    def get_feats(self, data:np.array):\n",
        "        serial, daily, weekly, y = [], [], [], []\n",
        "        start_idx = max(self.serial_len, self.daily_len*self.day_timesteps, self.weekly_len*self.day_timesteps * 7)\n",
        "        end_idx = data.shape[0] - self.pre_horizon + 1\n",
        "        for i in range(start_idx, end_idx):\n",
        "            serial.append(data[i-self.serial_len : i])\n",
        "            daily.append(self.get_periodic_skip_seq(data, i, 'daily'))\n",
        "            weekly.append(self.get_periodic_skip_seq(data, i, 'weekly'))\n",
        "            y.append(data[i:i+self.pre_horizon])\n",
        "        return np.array(serial), np.array(daily), np.array(weekly), np.array(y)\n",
        "\n",
        "    def get_periodic_skip_seq(self, data:np.array, idx:int, p:str):\n",
        "        p_seq = list()\n",
        "        if p == 'daily':\n",
        "            p_steps = self.daily_len * self.day_timesteps\n",
        "            for d in range(1, self.daily_len+1):\n",
        "                p_seq.append(data[idx - p_steps*d])\n",
        "        else:   # weekly\n",
        "            p_steps = self.weekly_len * self.day_timesteps * 7\n",
        "            for w in range(1, self.weekly_len+1):\n",
        "                p_seq.append(data[idx - p_steps*w])\n",
        "        p_seq = p_seq[::-1]     # inverse order\n",
        "        return np.array(p_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhiNvWtZKgQJ"
      },
      "source": [
        "# Model Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSRi9MO1KFGr"
      },
      "source": [
        "import datetime\n",
        "class ModelTrainer(object):\n",
        "    def __init__(self, model_name: str, model:nn.Module, loss:nn.Module, optimizer, lr:float, wd:float, n_epochs:int, n_node1: int, n_node2: int, a=0.5):\n",
        "        self.model = model\n",
        "        self.model_name = model_name\n",
        "        self.criterion = loss\n",
        "        self.optimizer = optimizer(params=self.model.parameters(), lr=lr, weight_decay=wd)\n",
        "        self.n_epochs = n_epochs\n",
        "        self.n_node1 = n_node1\n",
        "        self.n_node2 = n_node2\n",
        "        self.a = a\n",
        "\n",
        "    def train(self, data_loader:dict, modes:list, model_dir:str, data_class, early_stopper=100):\n",
        "        checkpoint = {'epoch':0, 'state_dict':self.model.state_dict()}\n",
        "        val_loss = np.inf\n",
        "\n",
        "        start_time = datetime.datetime.now()\n",
        "        for epoch in range(1, self.n_epochs+1):\n",
        "\n",
        "            running_loss = {mode:0.0 for mode in modes}\n",
        "            for mode in modes:\n",
        "                if mode == 'train':\n",
        "                    self.model.train()\n",
        "                else:\n",
        "                    self.model.eval()\n",
        "\n",
        "                step = 0\n",
        "                for x, y_true in data_loader[mode]:\n",
        "                    # print('x', x.size(), 'y_true', y_true.size())\n",
        "                    with torch.set_grad_enabled(mode = mode=='train'):\n",
        "                        y_pred1, y_pred2 = self.model(x)\n",
        "                        y_true1, y_true2 = y_true[:, :, :self.n_node1, :], y_true[:, :, self.n_node1:, :]\n",
        "                        loss = self.a * self.criterion(y_pred1, y_true1) + (1-self.a) * self.criterion(y_pred2, y_true2)\n",
        "                        if mode == 'train':\n",
        "                            self.optimizer.zero_grad()\n",
        "                            loss.backward()\n",
        "                            self.optimizer.step()\n",
        "                    running_loss[mode] += loss * y_true.shape[0]\n",
        "                    step += y_true.shape[0]\n",
        "\n",
        "                # epoch end\n",
        "                if mode == 'validate':\n",
        "                    if running_loss[mode]/step <= val_loss:\n",
        "                        # print(f'Epoch {epoch}, Val_loss drops from {val_loss:.5} to {running_loss[mode]/step:.5}. '\n",
        "                        #       f'Update model checkpoint..')\n",
        "                        val_loss = running_loss[mode]/step\n",
        "                        checkpoint.update(epoch=epoch, state_dict=self.model.state_dict())\n",
        "                        torch.save(checkpoint, model_dir + f'/{self.model_name}_best_model.pkl')\n",
        "                        early_stopper = 100\n",
        "                    else:\n",
        "                        # print(f'Epoch {epoch}, Val_loss does not improve from {val_loss:.5}.')\n",
        "                        early_stopper -= 1\n",
        "                        if early_stopper == 0:\n",
        "                            print(f'Early stopping at epoch {epoch}..')\n",
        "                            return\n",
        "            if epoch % 50 == 0: \n",
        "                self.test(epoch=epoch, data_loader=data_loader, modes=['train', 'test'], model_dir=model_dir, data_class=data_in)\n",
        "        # print('Training ends at: ', time.ctime())\n",
        "        torch.save(checkpoint, model_dir + f'/{self.model_name}_best_model.pkl')\n",
        "        print('training', datetime.datetime.now() - start_time)\n",
        "        return\n",
        "\n",
        "    # since cuda always runs out of memory, we will conduct test on cpu.\n",
        "    def test(self, epoch, data_loader:dict, modes:list, model_dir:str, data_class):\n",
        "\n",
        "        saved_checkpoint = torch.load(model_dir + f'/{self.model_name}_best_model.pkl')\n",
        "        self.model.load_state_dict(saved_checkpoint['state_dict'])\n",
        "        self.model.eval()\n",
        "\n",
        "        # print('Testing starts at: ', time.ctime())\n",
        "        running_loss = {mode: 0.0 for mode in modes}\n",
        "        start_time = datetime.datetime.now()\n",
        "        for mode in modes:\n",
        "            ground_truth1, prediction1 = list(), list()\n",
        "            ground_truth2, prediction2 = list(), list()\n",
        "            for x, y_true in data_loader[mode]:\n",
        "                y_pred1, y_pred2 = self.model(x)\n",
        "                y_true1, y_true2 = y_true[:, :, :self.n_node1, :], y_true[:, :, self.n_node1:, :]\n",
        "\n",
        "                ground_truth1.append(y_true1.cpu().detach().numpy())\n",
        "                prediction1.append(y_pred1.cpu().detach().numpy())\n",
        "\n",
        "                ground_truth2.append(y_true2.cpu().detach().numpy())\n",
        "                prediction2.append(y_pred2.cpu().detach().numpy())\n",
        "\n",
        "            ground_truth1 = data_class.minmax_denormalize(np.concatenate(ground_truth1, axis=0), data_class.min1, data_class.max1)\n",
        "            prediction1 = data_class.minmax_denormalize(np.concatenate(prediction1, axis=0), data_class.min1, data_class.max1)\n",
        "            print('Mode1', f'{epoch}{mode} RMSE: ', self.RMSE(prediction1, ground_truth1), 'MAE:', self.MAE(prediction1, ground_truth1), 'MAPE:', self.MAPE(prediction1, ground_truth1), 'PCC:', self.PCC(prediction1, ground_truth1))\n",
        "\n",
        "            ground_truth2 = data_class.minmax_denormalize(np.concatenate(ground_truth2, axis=0), data_class.min2, data_class.max2)\n",
        "            prediction2 = data_class.minmax_denormalize(np.concatenate(prediction2, axis=0), data_class.min2, data_class.max2)\n",
        "            print('Mode2', f'{epoch}{mode} RMSE: ', self.RMSE(prediction2, ground_truth2), 'MAE:', self.MAE(prediction2, ground_truth2), 'MAPE:', self.MAPE(prediction2, ground_truth2), 'PCC:', self.PCC(prediction2, ground_truth2))\n",
        "        return\n",
        "\n",
        "    @staticmethod\n",
        "    def MSE(y_pred:np.array, y_true:np.array):\n",
        "        return np.mean(np.square(y_pred - y_true))\n",
        "    @staticmethod\n",
        "    def RMSE(y_pred:np.array, y_true:np.array):\n",
        "        return np.sqrt(np.mean(np.square(y_pred - y_true)))\n",
        "    @staticmethod\n",
        "    def MAE(y_pred:np.array, y_true:np.array):\n",
        "        return np.mean(np.abs(y_pred - y_true))\n",
        "    @staticmethod\n",
        "    def MAPE(y_pred:np.array, y_true:np.array, epsilon=1e-0):   # zero division\n",
        "        return np.mean(np.abs(y_pred - y_true) / (y_true + epsilon))\n",
        "    @staticmethod\n",
        "    def PCC(y_pred:np.array, y_true:np.array):\n",
        "        return np.corrcoef(y_pred.flatten(), y_true.flatten())[0,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf2YhcGAM5ie"
      },
      "source": [
        "# Model Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW-k4XUUT4qk"
      },
      "source": [
        "\"\"\"load demand data\"\"\"\n",
        "import numpy as np\n",
        "M_adj = 1 # num static adjs\n",
        "\n",
        "data_path1 = \"../data/metro_demand_4h.npy\"\n",
        "data_path2 = \"../data/FR_demand_4h.npy\"\n",
        "\n",
        "data_in = DataInput(data_dir1=data_path1, data_dir2=data_path2, norm_opt=True)\n",
        "data = data_in.load_data()\n",
        "n_node1, n_node2 = data_in.n_node1, data_in.n_node2\n",
        "print(data_in.n_node1, data_in.n_node2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gP52wtbK3ZO"
      },
      "source": [
        "\"\"\"load adj data\"\"\"\n",
        "bb_adj_path = \"../data/relationship/geo/Metro_adj.npy\"\n",
        "tt_adj_path = \"../data/relationship/geo/Unit_adj.npy\"\n",
        "bt_adj_path =  \"../data/relationship/geo/Metro_Unit_adj.npy\"\n",
        "\n",
        "A = []\n",
        "A.append(np.load(bb_adj_path))\n",
        "A.append(np.load(bt_adj_path))\n",
        "A.append(np.load(tt_adj_path))\n",
        "A.append(np.transpose(np.load(bt_adj_path)))\n",
        "assert len(A) == 4\n",
        "\n",
        "bb_pat_path = \"../data/relationship/pattern/Metro_adj.npy\"\n",
        "tt_pat_path = \"../data/relationship/pattern/Unit_adj.npy\"\n",
        "bt_pat_path =  \"../data/relationship/pattern/Metro_Unit_adj.npy\"\n",
        "\n",
        "A_pat = []\n",
        "A_pat.append(np.load(bb_pat_path))\n",
        "A_pat.append(np.load(bt_pat_path))\n",
        "A_pat.append(np.load(tt_pat_path))\n",
        "A_pat.append(np.transpose(np.load(bt_pat_path)))\n",
        "assert len(A_pat) == 4\n",
        "\n",
        "mat_type = 'hat_rw_normd_lap_mat'\n",
        "gcnconv_matrix_list = []\n",
        "for adj_mat, pat_mat in zip(A, A_pat):\n",
        "    A_adj = torch.from_numpy((calculate_random_walk_matrix(adj_mat.T).T).astype('float32')).to(device) # This is for D-GCN\n",
        "    A_pat = torch.from_numpy((calculate_random_walk_matrix(pat_mat.T).T).astype('float32')).to(device)\n",
        "    A_h = torch.cat([A_adj.unsqueeze(0), A_pat.unsqueeze(0)], 0)\n",
        "    gcnconv_matrix_list.append(A_h)\n",
        "    print(adj_mat.shape)\n",
        "    print(A_h.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "benAh6V2M1t3"
      },
      "source": [
        "\"\"\"prepare data for model training and testing\"\"\"\n",
        "dt = 0.25\n",
        "\n",
        "obs_len = (6, 0, 0) # short-term/daily/weekly observations\n",
        "\n",
        "pre_horizon = 1\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# data_generator = DataGenerator(dt=dt, obs_len=obs_len, train_test_dates=dates, val_ratio=0.2, test_ratio=0.2)\n",
        "data_generator = DataGenerator(dt=dt, obs_len=obs_len, pre_horizon=pre_horizon)\n",
        "\n",
        "data_loader = data_generator.get_data_loader(data=data, batch_size=batch_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAwrYirXfU1Y"
      },
      "source": [
        "\"\"\"hyperparameter settings\"\"\"\n",
        "Ks = 2\n",
        "Kt = 2\n",
        "\n",
        "epoch = 500\n",
        "\n",
        "learn_rate, weight_decay = 2e-3, 1e-5\n",
        "\n",
        "dropout = 0.3\n",
        "\n",
        "stblock_num = 2\n",
        "\n",
        "n_his = sum(obs_len)\n",
        "Ko = n_his - (Kt - 1) * 2 * stblock_num\n",
        "\n",
        "blocks = []\n",
        "blocks.append([2])\n",
        "for l in range(stblock_num):\n",
        "    blocks.append([64, 16, 64])\n",
        "if Ko == 0:\n",
        "    blocks.append([128])\n",
        "elif Ko > 0:\n",
        "    blocks.append([128, 128])\n",
        "blocks.append([2])\n",
        "\n",
        "gated_act_func = 'glu' # 'glu' for GLU, 'gtu' for GTU (temporal convolution)\n",
        "\n",
        "graph_conv_type = 'gcnconv' # 'chebconv' for STGCN_ChebConv, 'gcnconv' for STGCN_GCNConv\n",
        "\n",
        "loss = nn.MSELoss(reduction='sum')\n",
        "   \n",
        "optimizer = optim.Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T42UZjpJwkrt"
      },
      "source": [
        "for i in range(10):\n",
        "\n",
        "    model_name = 'STMRGNN_%d'%i\n",
        "\n",
        "    model = STGCN_ChebConv(Kt, Ks, blocks, n_his, n_node1, n_node2, gated_act_func, graph_conv_type, gcnconv_matrix_list, dropout).to(device)\n",
        "\n",
        "    trainer = ModelTrainer(model_name=model_name, model=model, loss=loss, optimizer=optimizer, lr=learn_rate, wd=weight_decay, n_epochs=epoch, n_node1=n_node1, n_node2=n_node2, a=0.5)\n",
        "\n",
        "    model_dir = \"../model\"\n",
        "\n",
        "    trainer.train(data_loader=data_loader, modes=['train', 'validate'], model_dir=model_dir, data_class=data_in)\n",
        "\n",
        "    trainer.test(epoch=i, data_loader=data_loader, modes=['test'], model_dir=model_dir, data_class=data_in)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}